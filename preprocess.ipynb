{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-11-08T19:03:35.874841Z",
     "start_time": "2023-11-08T19:03:35.869554Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from pytorch3d.transforms import quaternion_invert, quaternion_apply, axis_angle_to_quaternion\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "force_cpu = True\n",
    "if not force_cpu:\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "        device = torch.device(\"mps\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(\"Using device\", device)\n",
    "\n",
    "\n",
    "class JointDataset(Dataset):\n",
    "    def __init__(self, x, joint_names):\n",
    "        super().__init__()\n",
    "        self.joint_names = joint_names\n",
    "        self.x = self._transform(self._scale(x))\n",
    "        self.furthest_distance = None\n",
    "        self.centroid = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.x.size(0)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx]\n",
    "\n",
    "    def _scale(self, x):\n",
    "        # Center the points and reduce to unit-sphere\n",
    "        x = x.reshape(-1, 3)\n",
    "        self.centroid = torch.mean(x, axis=0)\n",
    "        x -= self.centroid\n",
    "        self.furthest_distance = torch.max(torch.sqrt(torch.sum(x ** 2, axis=-1)))\n",
    "        x /= self.furthest_distance\n",
    "        # Put back into the original shape\n",
    "        x = x.reshape(-1, 32, 3)\n",
    "        return x\n",
    "\n",
    "    def _unscale(self, x):\n",
    "        if self.furthest_distance is None:\n",
    "            raise ValueError(\"Dataset has not been scaled yet\")\n",
    "        x *= self.furthest_distance\n",
    "        x += self.centroid\n",
    "        return x\n",
    "    \n",
    "    def _transform(self, x):\n",
    "        \"\"\"\n",
    "            Compute the transformation matrix that transforms the reference points into the target points.\n",
    "            references = torch.from_numpy(np.random.randint(0, 10, size=((n_obs, 3)))).float()\n",
    "            targets = torch.from_numpy(np.random.randint(0, 10, size=((n_obs, n_joints, 3)))).float()\n",
    "        \"\"\"\n",
    "        \n",
    "        pelvis_idx = list(self.joint_names).index(\"PELVIS\")\n",
    "        \n",
    "        # Get the references\n",
    "        references = x[:, pelvis_idx]\n",
    "        \n",
    "        # Split the tensor into two parts, before and after the pelvis_idx\n",
    "        first_part = x[:, :pelvis_idx]\n",
    "        second_part = x[:, pelvis_idx+1:]\n",
    "        \n",
    "        # Stack the two parts together\n",
    "        targets = torch.cat((first_part, second_part), dim=1)\n",
    "        \n",
    "        print(\"targets shape\", targets.shape)\n",
    "        print(\"references shape\", references.shape)\n",
    "                \n",
    "        n_joints = targets.size(1)\n",
    "        \n",
    "        # Normalize A and B\n",
    "        normalized_A = references / torch.norm(references, dim=-1, keepdim=True)\n",
    "        normalized_B = targets / torch.norm(targets, dim=-1, keepdim=True)\n",
    "        \n",
    "        # Calculate the axis of rotation\n",
    "        axis = torch.cross(normalized_A.unsqueeze(1), normalized_B)\n",
    "        axis = axis / torch.norm(axis, dim=-1, keepdim=True)\n",
    "        \n",
    "        # The angle of rotation is the arccosine of the dot product of the normalized vectors\n",
    "        angle_of_rotation = torch.acos((normalized_A.unsqueeze(1) * normalized_B).sum(-1))\n",
    "        \n",
    "        # The axis-angle representation is the axis of rotation scaled by the angle of rotation\n",
    "        axis_angle = axis * angle_of_rotation.unsqueeze(-1)\n",
    "        \n",
    "        # Convert the axis-angle representation to a quaternion\n",
    "        rotation = axis_angle_to_quaternion(axis_angle)\n",
    "        \n",
    "        # Apply the rotation to point A\n",
    "        A_rotated = quaternion_apply(rotation, references.unsqueeze(1).expand(-1, n_joints, -1))\n",
    "        \n",
    "        # Compute the translation vector\n",
    "        translation = targets - A_rotated\n",
    "        \n",
    "        # Apply the translation to point A\n",
    "        A_transformed = A_rotated + translation\n",
    "\n",
    "        transformations = torch.cat((rotation, translation), dim=-1)\n",
    "        flatten_transformations = transformations.reshape(transformations.size(0), -1)\n",
    "        \n",
    "        print(\"transformations shape\", flatten_transformations.shape)\n",
    "        print(\"references shape\", references.shape)\n",
    "        data = torch.cat((references, flatten_transformations), dim=-1)\n",
    "        print(\"data shape\", data.shape)\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def _untransform(self, x):\n",
    "        \n",
    "        transformations = torch.split(x, 4, dim=-1) \n",
    "\n",
    "        print(transformations.shape)\n",
    "        rotation, translation = torch.split(transformation, 4, dim=-1)\n",
    "        # Apply the rotation to point A\n",
    "        A_rotated = quaternion_apply(rotation, references.unsqueeze(1).expand(-1, n_joints, -1))\n",
    "\n",
    "        # Compute the translation vector\n",
    "        translation = targets - A_rotated\n",
    "\n",
    "        # Apply the translation to point A\n",
    "        A_transformed = A_rotated + translation\n",
    "        \n",
    "        \n",
    "    def untransform_and_unscale(self):\n",
    "        x = self._untransform(self.x)\n",
    "        x = self._unscale(x)\n",
    "        return x\n",
    "\n",
    "def make_joint_dataset(device: torch.device):\n",
    "    with open(\"data/unlabelled/camera/joints/front_sit_stand.csv\") as f:\n",
    "        data = pd.read_csv(f, header=0)\n",
    "        data = data.rename(columns={\"x-axis\": \"x\", \"y-axis\": \"y\", \"z-axis\": \"z\", \"joint_names\": \"joint_name\"})\n",
    "        # print(data.head())\n",
    "\n",
    "    # Get the unique joint names and frame IDs\n",
    "    joint_names = np.sort(data[\"joint_name\"].unique())\n",
    "    frame_ids = data[\"frame_id\"].unique()\n",
    "\n",
    "    # Create a multi-index using 'frame_id' and 'joint_names'\n",
    "    data.set_index(['frame_id', 'joint_name'], inplace=True)\n",
    "\n",
    "    # Sort the index to ensure the data is in the correct order\n",
    "    data.sort_index(inplace=True)\n",
    "\n",
    "    # Convert the DataFrame to a NumPy array and reshape it\n",
    "    x = data[['x', 'y', 'z']].values.reshape((frame_ids.size, joint_names.size, 3))\n",
    "\n",
    "    # Convert the NumPy array to a PyTorch tensor\n",
    "    x = torch.from_numpy(x).float().to(device)\n",
    "\n",
    "    dataset = JointDataset(joint_names=joint_names, x=x)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets shape torch.Size([2520, 31, 3])\n",
      "references shape torch.Size([2520, 3])\n",
      "transformations shape torch.Size([2520, 217])\n",
      "references shape torch.Size([2520, 3])\n",
      "data shape torch.Size([2520, 220])\n"
     ]
    }
   ],
   "source": [
    "trainings_data = make_joint_dataset(device)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-08T19:03:36.903642Z",
     "start_time": "2023-11-08T19:03:36.826174Z"
    }
   },
   "id": "ac2b360015e240d9"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bdc27ee6592a9902"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
